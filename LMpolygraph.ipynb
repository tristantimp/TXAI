{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trist_243cr5c\\Documents\\block 3\\TXAI\\lm-polygraph\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from lm_polygraph.estimators import *\n",
    "from lm_polygraph.utils.model import WhiteboxModel\n",
    "from lm_polygraph.utils.dataset import Dataset\n",
    "from lm_polygraph.utils.processor import Logger\n",
    "from lm_polygraph.utils.manager import UEManager\n",
    "from lm_polygraph.ue_metrics import PredictionRejectionArea\n",
    "from lm_polygraph.generation_metrics import RougeMetric, BartScoreSeqMetric, ModelScoreSeqMetric, ModelScoreTokenwiseMetric, AggregatedMetric\n",
    "from lm_polygraph.utils.builder_enviroment_stat_calculator import (\n",
    "    BuilderEnvironmentStatCalculator\n",
    ")\n",
    "from lm_polygraph.defaults.register_default_stat_calculators import (\n",
    "    register_default_stat_calculators,\n",
    ")\n",
    "from lm_polygraph.utils.factory_stat_calculator import StatCalculatorContainer\n",
    "from omegaconf import OmegaConf\n",
    "from lm_polygraph import estimate_uncertainty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m', device_map='cpu',)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/bloomz-560m')\n",
    "model = WhiteboxModel(base_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=array([-0.16278946, -0.40806034, -0.38379776], dtype=float32), input_text='Qui est George Bush?', generation_text=' le président américain', generation_tokens=[578, 17635, 52762], model_path=None, estimator='MaximumTokenProbability')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MaximumTokenProbability()\n",
    "estimate_uncertainty(model, estimator, input_text='Qui est George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.7690080404281616],\n",
       " 'recall': [0.7674813270568848],\n",
       " 'f1': [0.7682439088821411],\n",
       " 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.50.1)'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"de kat slaapt op de bank\"]\n",
    "references = [\"een poes ligt op een sofa\"]\n",
    "bertscore.compute(predictions=predictions, references=references, lang=\"nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_type = \"Whitebox\"\n",
    "dataset_name = (\"trivia_qa\", \"rc.nocontext\")\n",
    "batch_size = 4\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load(\n",
    "    dataset_name,\n",
    "    'question', 'answer',\n",
    "    batch_size=batch_size,\n",
    "    prompt=\"Question: {question}\\nAnswer:{answer}\",\n",
    "    split=\"validation\"\n",
    ")\n",
    "dataset.subsample(1, seed=seed)\n",
    "\n",
    "train_dataset = Dataset.load(\n",
    "    dataset_name,\n",
    "    'question', 'answer',\n",
    "    batch_size=batch_size,\n",
    "    prompt=\"Question: {question}\\nAnswer:{answer}\",\n",
    "    split=\"train\"\n",
    ")\n",
    "train_dataset.subsample(1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_methods = [MaximumSequenceProbability(), \n",
    "              SemanticEntropy(),\n",
    "              MahalanobisDistanceSeq(\"decoder\"),\n",
    "             ]\n",
    "\n",
    "ue_metrics = [PredictionRejectionArea(), PredictionRejectionArea(max_rejection=0.5)]\n",
    "\n",
    "# Wrap generation metric in AggregatedMetric, since trivia_qa is a multi-reference dataset\n",
    "# (y is a list of possible correct answers)\n",
    "metrics = [AggregatedMetric(RougeMetric('rougeL'))]\n",
    "\n",
    "loggers = [Logger()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingStatistic_config = {\n",
    "    \"dataset\": dataset_name,\n",
    "    \"text_column\": 'question',\n",
    "    \"label_column\": 'answer',\n",
    "    \"description\": '',\n",
    "    \"prompt\": \"Question: {question}\\nAnswer:\",\n",
    "    \"few_shot_split\": 'train',\n",
    "    \"train_split\": 'train',\n",
    "    \"load_from_disk\": False,\n",
    "    \"subsample_train_dataset\": 10,\n",
    "    \"n_shot\": 5,\n",
    "    \"train_dataset\": dataset_name,\n",
    "    \"train_test_split\": False,\n",
    "    \"background_train_dataset\": 'allenai/c4',\n",
    "    \"background_train_dataset_text_column\": 'text',\n",
    "    \"background_train_dataset_label_column\": 'url',\n",
    "    \"background_train_dataset_data_files\": 'en/c4-train.00000-of-01024.json.gz',\n",
    "    \"background_load_from_disk\": False,\n",
    "    \"subsample_background_train_dataset\": 10,\n",
    "    \"batch_size\": 1,\n",
    "    \"seed\": 1,\n",
    "    \"size\": 1,\n",
    "    \"bg_size\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_stat_calculators = dict()\n",
    "scs = register_default_stat_calculators(model_type)\n",
    "for sc in scs:\n",
    "    result_stat_calculators[sc.name] = sc\n",
    "\n",
    "# register TrainingStatisticExtractionCalculator for the Mahalanobis Distance method\n",
    "result_stat_calculators.update(\n",
    "    {\n",
    "        \"TrainingStatisticExtractionCalculator\": StatCalculatorContainer(\n",
    "            name=\"TrainingStatisticExtractionCalculator\",\n",
    "            cfg=OmegaConf.create(TrainingStatistic_config),\n",
    "            stats=[\"train_embeddings\", \"background_train_embeddings\", \"train_greedy_log_likelihoods\"],\n",
    "            dependencies=[],\n",
    "            builder=\"lm_polygraph.defaults.stat_calculator_builders.default_TrainingStatisticExtractionCalculator\",\n",
    "        )\n",
    "    }\n",
    ")\n",
    "    \n",
    "builder_env_stat_calc = BuilderEnvironmentStatCalculator(model=model)\n",
    "available_stat_calculators = list(result_stat_calculators.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Generating train split: 356317 examples [00:06, 59020.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "man = UEManager(\n",
    "    data=dataset,\n",
    "    model=model,\n",
    "    estimators=ue_methods,\n",
    "    builder_env_stat_calc=builder_env_stat_calc,\n",
    "    available_stat_calculators=available_stat_calculators,\n",
    "    generation_metrics=metrics,\n",
    "    ue_metrics=ue_metrics,\n",
    "    processors=loggers,\n",
    "    ignore_exceptions=False,\n",
    "    max_new_tokens=10, \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n",
      "c:\\Users\\trist_243cr5c\\Documents\\block 3\\TXAI\\lm-polygraph\\.env\\Lib\\site-packages\\lm_polygraph\\estimators\\mahalanobis_distance.py:35: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Correlation.cpp:116.)\n",
      "  cov_scaled = torch.cov(train_features.T)\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.14s/it]\n",
      "WARNING:lm_polygraph:We got 1 nans in MahalanobisDistanceSeq_decoder estimator.\n",
      "c:\\Users\\trist_243cr5c\\Documents\\block 3\\TXAI\\lm-polygraph\\.env\\Lib\\site-packages\\lm_polygraph\\ue_metrics\\pred_rej_area.py:52: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prr_score = np.sum(scores) / num_rej\n",
      "WARNING:lm_polygraph:We got 1 nans in MahalanobisDistanceSeq_decoder estimator.\n",
      "  0%|          | 0/1 [00:17<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "results = man()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr, Score: 0.500\n",
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr_normalized, Score: 0.500\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr, Score: 0.500\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr_normalized, Score: 0.500\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr, Score: 0.500\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr_normalized, Score: 0.500\n",
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr_0.5, Score: nan\n",
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr_0.5_normalized, Score: nan\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr_0.5, Score: nan\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr_0.5_normalized, Score: nan\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr_0.5, Score: nan\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr_0.5_normalized, Score: nan\n"
     ]
    }
   ],
   "source": [
    "for key in results.keys():\n",
    "    print(f\"UE Score: {key[1]}, Metric: {key[2]}, UE Metric: {key[3]}, Score: {results[key]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
